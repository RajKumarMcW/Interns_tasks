{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1003830,"sourceType":"datasetVersion","datasetId":550917}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_recall_fscore_support, roc_curve, auc, precision_score,recall_score, f1_score\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_file_path = \"/kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Cat\"\ndog_file_path = \"/kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nclass_names = ['Cat', 'Dog'] \nn_dogs = len(os.listdir(cat_file_path))\nn_cats = len(os.listdir(dog_file_path))\nn_images = [n_cats, n_dogs]\n\nfig = px.bar(x=class_names, y=n_images, color=class_names, labels={'x': 'Class', 'y': 'Number of Images'}, title='Dog vs Cat')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img in os.listdir(cat_file_path):\n        img_path = os.path.join(cat_file_path, img)\n        label = 0\n        arr = cv2.imread(img_path)\n        if arr is None:\n            print(f\"Error loading image: {img_path}\")\n        else:\n            augmented_image = random_rotation(arr)\n            augmented_image = horizontal_flip(augmented_image)\n            augmented_image = vertical_flip(augmented_image)\n            augmented_image = cv2.resize(augmented_image, (150, 150))\n            new_arr = cv2.resize(arr, (150, 150))\n            data.append([new_arr, label])\n            data.append([augmented_image, label])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"            \nfor img in os.listdir(dog_file_path):\n        img_path = os.path.join(dog_file_path, img)\n        label = 1\n        arr = cv2.imread(img_path)\n        if arr is None:\n            print(f\"Error loading image: {img_path}\")\n        else:\n            # Apply transformations\n            augmented_image = random_rotation(arr)\n            augmented_image = horizontal_flip(augmented_image)\n            augmented_image = vertical_flip(augmented_image)\n            augmented_image = cv2.resize(augmented_image, (150, 150))\n            new_arr = cv2.resize(arr, (150, 150))\n            data.append([new_arr, label])\n            data.append([augmented_image, label])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data,test_data  = train_test_split(data, train_size=0.8, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = []\ny_train = []\n\nfor features,labels in train_data:\n    X_train.append(features)\n    y_train.append(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = []\ny_test = []\n\nfor features,labels in test_data:\n    X_test.append(features)\n    y_test.append(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = np.array(X_test)\ny_test = np.array(y_test)\nX_train = np.array(X_train)\ny_test = np.array(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_data\ndel train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"del data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape((len(X_train), 150*150*3))\nX_test = X_test.reshape((len(X_test), 150*150*3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train/255\nX_test = X_test/255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_categorical(labels, num_classes=2):\n    one_hot_labels = np.zeros((len(labels), num_classes))\n\n    for i, label in enumerate(labels):\n        one_hot_labels[i, label] = 1\n\n    return one_hot_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = convert_to_categorical(y_train, 2)\ny_test = convert_to_categorical(y_test, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ANN:\n  def __init__(self):\n\n    self.learning_rate = 0.01\n    self.epochs = 50\n\n    # Neural network architecture parameters\n    self.input_size = 150*150*3  # 150*150\n    self.hidden_size = 5\n    self.output_size = 5  \n\n    #Initialize weights and biases\n    np.random.seed(42)\n    self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n    self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n    self.bias_hidden = np.zeros((1, self.hidden_size))\n    self.bias_output = np.zeros((1, self.output_size))\n\n  def sigmoid(self,x):\n    return 1 / (1 + np.exp(-x))\n\n  def sigmoid_derivative(self,x):\n      return x * (1 - x)\n\n  def binary_cross_entropy(self,y_true, y_pred):\n    # Avoid division by zero\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\n\n  def train(self,x_train,y_train):\n      # Training the network\n      X_train = x_train\n      y_train = y_train\n      for e in range(self.epochs):\n          total_loss = 0\n          for i in range(len(X_train)):\n              # Forward pass\n              input_layer = np.array([X_train[i]])\n              hidden_layer = self.sigmoid(np.dot(input_layer, self.weights_input_hidden) + self.bias_hidden)\n              output_layer = self.sigmoid(np.dot(hidden_layer, self.weights_hidden_output) + self.bias_output)\n              # Backpropagation\n              loss = self.binary_cross_entropy(y_test,output_layer)\n              total_loss += loss\n              output_error = y_train[i] - output_layer\n              output_delta = output_error * self.sigmoid_derivative(output_layer)\n\n              hidden_error = output_delta.dot(self.weights_hidden_output.T)\n              hidden_delta = hidden_error * self.sigmoid_derivative(hidden_layer)\n\n              # Update weights and biases\n              self.weights_hidden_output += hidden_layer.T.dot(output_delta) * self.learning_rate\n              self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n              self.weights_input_hidden += input_layer.T.dot(hidden_delta) * self.learning_rate\n              self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n            \n          # Output epoch results\n          total_loss /= len(X_train)\n          print(f\"Epoch {e+1}/{self.epochs} loss - {total_loss}\")\n\n  def evaluate(self,X_test,y_test):\n      # Evaluating the model\n      correct_predictions = 0\n      pred = []\n      for i in range(len(X_test)):\n          input_layer = np.array([X_test[i]])\n          hidden_layer = self.sigmoid(np.dot(input_layer, self.weights_input_hidden) + self.bias_hidden)\n          output_layer = self.sigmoid(np.dot(hidden_layer, self.weights_hidden_output) + self.bias_output)\n          pred.append(output_layer)\n          if np.argmax(output_layer) == np.argmax(y_test[i]):\n              correct_predictions += 1\n\n      print(f\"Test accuracy: {correct_predictions / len(y_test)}\")\n      return pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_model = ANN()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_model.train(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = ann_model.evaluate(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = np.array(pred).squeeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top1_accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(pred, axis=1))\nprint(f\"Accuracy: {top1_accuracy * 100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision, recall, f1_score, _ = precision_recall_fscore_support(np.argmax(y_test, axis=1), np.argmax(pred, axis=1), average='weighted')\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1_score:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(pred, axis=1))\nprint(\"Confusion Matrix:\\n\", conf_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 2\nTP = np.zeros(num_classes)\nTN = np.zeros(num_classes)\nFP = np.zeros(num_classes)\nFN = np.zeros(num_classes)\n\nfor cls in range(num_classes):\n    TP[cls] = conf_matrix[cls, cls]\n    TN[cls] = np.sum(conf_matrix) - np.sum(conf_matrix[cls, :]) - np.sum(conf_matrix[:, cls]) + conf_matrix[cls, cls]\n    FP[cls] = np.sum(conf_matrix[:, cls]) - conf_matrix[cls, cls]\n    FN[cls] = np.sum(conf_matrix[cls, :]) - conf_matrix[cls, cls]\n\nfor cls in range(num_classes):\n    print(f\"Class {cls}:\")\n    print(f\"  True Positives (TP): {TP[cls]}\")\n    print(f\"  True Negatives (TN): {TN[cls]}\")\n    print(f\"  False Positives (FP): {FP[cls]}\")\n    print(f\"  False Negatives (FN): {FN[cls]}\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = y_test.shape[1]\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plot ROC curves\nplt.figure(figsize=(10, 8))\nlw = 2\nfor i, color in zip(range(n_classes),['aqua', 'darkorange']):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], '--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Multiclass ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import mean\n",
        "from typing import Dict, List, Tuple"
      ],
      "metadata": {
        "id": "9-J_JXlj7hFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2FktUuY7Rch"
      },
      "outputs": [],
      "source": [
        "class Neural:\n",
        "\n",
        "    def __init__(self, layers: List[int], epochs: int,\n",
        "                 learning_rate: float = 0.001, batch_size: int=32,\n",
        "                 validation_split: float = 0.2, verbose: int=1):\n",
        "        self._layer_structure: List[int] = layers\n",
        "        self._batch_size: int = batch_size\n",
        "        self._epochs: int = epochs\n",
        "        self._learning_rate: float = learning_rate\n",
        "        self._validation_split: float = validation_split\n",
        "        self._verbose: int = verbose\n",
        "        self._losses: Dict[str, float] = {\"train\": [], \"validation\": []}\n",
        "        self._is_fit: bool = False\n",
        "        self.__layers = None\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        # validation split\n",
        "        X, X_val, y, y_val = train_test_split(X, y, test_size=self._validation_split, random_state=42)\n",
        "        # initialization of layers\n",
        "        self.__layers = self.__init_layers()\n",
        "        for epoch in range(self._epochs):\n",
        "            epoch_losses = []\n",
        "            for i in range(1, len(self.__layers)):\n",
        "                # forward pass\n",
        "                x_batch = X[i:(i+self._batch_size)]\n",
        "                y_batch = y[i:(i+self._batch_size)]\n",
        "                pred, hidden = self.__forward(x_batch)\n",
        "                # calculate loss\n",
        "                loss = self.__calculate_loss(y_batch, pred)\n",
        "                epoch_losses.append(np.mean(loss ** 2))\n",
        "                #backward\n",
        "                self.__backward(hidden, loss)\n",
        "            valid_preds, _ = self.__forward(X_val)\n",
        "            train_loss = mean(epoch_losses)\n",
        "            valid_loss = np.mean(self.__calculate_mse(valid_preds,y_val))\n",
        "            self._losses[\"train\"].append(train_loss)\n",
        "            self._losses[\"validation\"].append(valid_loss)\n",
        "            if self._verbose:\n",
        "                print(f\"Epoch: {epoch} Train MSE: {train_loss} Valid MSE: {valid_loss}\")\n",
        "        self._is_fit = True\n",
        "        return\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self._is_fit == False:\n",
        "            raise Exception(\"Model has not been trained yet.\")\n",
        "        pred, hidden = self.__forward(X)\n",
        "        return pred\n",
        "\n",
        "    def plot_learning(self) -> None:\n",
        "        plt.plot(self._losses[\"train\"],label=\"loss\")\n",
        "        plt.plot(self._losses[\"validation\"],label=\"validation\")\n",
        "        plt.legend()\n",
        "\n",
        "    def __init_layers(self) -> List[np.ndarray]:\n",
        "        layers = []\n",
        "        for i in range(1, len(self._layer_structure)):\n",
        "            layers.append([\n",
        "                np.random.rand(self._layer_structure[i-1], self._layer_structure[i]) / 5 - .1,\n",
        "                np.ones((1,self._layer_structure[i]))\n",
        "            ])\n",
        "        return layers\n",
        "\n",
        "    def __forward(self, batch: np.ndarray) -> Tuple[np.ndarray, List[np.ndarray]]:\n",
        "        hidden = [batch.copy()]\n",
        "        for i in range(len(self.__layers)):\n",
        "            batch = np.matmul(batch, self.__layers[i][0]) + self.__layers[i][1]\n",
        "            if i < len(self.__layers) - 1:\n",
        "                batch = np.maximum(batch, 0)\n",
        "            # Store the forward pass hidden values for use in backprop\n",
        "            hidden.append(batch.copy())\n",
        "        return batch, hidden\n",
        "\n",
        "    def __calculate_loss(self,actual: np.ndarray, predicted: np.ndarray) -> np.ndarray:\n",
        "        \"mse\"\n",
        "        return predicted - actual\n",
        "\n",
        "\n",
        "    def __calculate_mse(self, actual: np.ndarray, predicted: np.ndarray) -> np.ndarray:\n",
        "        return (actual - predicted) ** 2\n",
        "\n",
        "    def __backward(self, hidden: List[np.ndarray], grad: np.ndarray) -> None:\n",
        "        for i in range(len(self.__layers)-1, -1, -1):\n",
        "            if i != len(self.__layers) - 1:\n",
        "                grad = np.multiply(grad, np.heaviside(hidden[i+1], 0))\n",
        "\n",
        "            w_grad = hidden[i].T @ grad\n",
        "            b_grad = np.mean(grad, axis=0)\n",
        "\n",
        "            self.__layers[i][0] -= w_grad * self._learning_rate\n",
        "            self.__layers[i][1] -= b_grad * self._learning_rate\n",
        "\n",
        "            grad = grad @ self.__layers[i][0].T\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data():\n",
        "    # Define correlation values\n",
        "    corr_a = 0.8\n",
        "    corr_b = 0.4\n",
        "    corr_c = -0.2\n",
        "\n",
        "    # Generate independent features\n",
        "    a = np.random.normal(0, 1, size=100000)\n",
        "    b = np.random.normal(0, 1, size=100000)\n",
        "    c = np.random.normal(0, 1, size=100000)\n",
        "    d = np.random.randint(0, 4, size=100000)\n",
        "    e = np.random.binomial(1, 0.5, size=100000)\n",
        "\n",
        "    # Generate target feature based on independent features\n",
        "    target = 50 + corr_a*a + corr_b*b + corr_c*c + d*10 + 20*e + np.random.normal(0, 10, size=100000)\n",
        "\n",
        "    # Create DataFrame with all features\n",
        "    df = pd.DataFrame({'a': a, 'b': b, 'c': c, 'd': d, 'e': e, 'target': target})\n",
        "    return df"
      ],
      "metadata": {
        "id": "xZAqqrE27ybN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "of-jp4gH70AV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
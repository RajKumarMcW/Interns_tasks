{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-X4rTzq5ZH9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/mnist_train.csv\")\n",
        "# print(df.describe())\n",
        "X_train=df.iloc[:,1:]\n",
        "y_train=df.iloc[:,0]\n",
        "X_train=X_train/255.\n",
        "X_train.shape,y_train.shape\n",
        "# print(X_train.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhihg-Uk6fd6",
        "outputId": "79c6440b-5a88-4b85-e24f-931aa95db34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784), (60000,))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_Y = np.zeros((y_train.size, y_train.max() + 1))\n",
        "one_hot_Y[np.arange(y_train.size), y_train] = 1\n",
        "# one_hot_Y = one_hot_Y.T\n",
        "one_hot_Y"
      ],
      "metadata": {
        "id": "TiHP-abFceBg",
        "outputId": "7dc58303-f48f-4f9a-fcc0-28faa939ae0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_img = X_train.reshape(19999, 28, 28)"
      ],
      "metadata": {
        "id": "xpBnXEL3_8XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(X_train_img[1])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "PPkjiHqx-JlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qHW8O6VQktqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        np.random.seed(42)\n",
        "        self.weights_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.biases_hidden = np.zeros((1, hidden_size))\n",
        "        self.weights_output = np.random.randn(hidden_size, output_size)\n",
        "        self.biases_output = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    # def cross_entropy_loss(self, y_true, y_pred):\n",
        "    #     return -np.sum(y_true * np.log(y_pred + 1e-10)) / len(y_true)\n",
        "\n",
        "    def cross_entropy_loss(self,y_true, y_pred):\n",
        "        return -np.sum(y_true * np.log(y_pred + 1e-10)) / len(y_true)\n",
        "\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        hidden_layer_input = np.dot(X, self.weights_hidden) + self.biases_hidden\n",
        "        hidden_layer_output = self.sigmoid(hidden_layer_input)\n",
        "        output_layer_input = np.dot(hidden_layer_output, self.weights_output) + self.biases_output\n",
        "        output_layer_output = self.softmax(output_layer_input)\n",
        "\n",
        "        return hidden_layer_output, output_layer_output\n",
        "\n",
        "    def backward_propagation(self, X, y_true, hidden_layer_output, output_layer_output):\n",
        "        output_error = output_layer_output - y_true\n",
        "        hidden_layer_error = np.dot(output_error, self.weights_output.T) * (hidden_layer_output * (1 - hidden_layer_output))\n",
        "\n",
        "        self.weights_output -= self.learning_rate * np.dot(hidden_layer_output.T, output_error)\n",
        "        self.biases_output -= self.learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
        "        self.weights_hidden -= self.learning_rate * np.dot(X.T, hidden_layer_error)\n",
        "        self.biases_hidden -= self.learning_rate * np.sum(hidden_layer_error, axis=0, keepdims=True)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=20):\n",
        "        for epoch in range(epochs):\n",
        "            # print(X_train.shape)\n",
        "            hidden_layer_output, output_layer_output = self.forward_propagation(X_train)\n",
        "\n",
        "            loss = self.cross_entropy_loss(y_train, output_layer_output)\n",
        "            self.backward_propagation(X_train, y_train, hidden_layer_output, output_layer_output)\n",
        "            # if epoch%10==0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss} , Accuracy: {self.evaluate(output_layer_output, y_train)}\")\n",
        "\n",
        "    def evaluate(self, output_layer_output_test, y_test):\n",
        "\n",
        "        predicted_labels = np.argmax(output_layer_output_test, axis=1)\n",
        "        true_labels = np.argmax(y_test, axis=1)\n",
        "        return np.mean(predicted_labels == true_labels)\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        _,output_layer_output_test = self.forward_propagation(X_test)\n",
        "        return output_layer_output_test"
      ],
      "metadata": {
        "id": "f02Vr8ZoZVez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[1]\n",
        "hidden_size = 100\n",
        "output_size = 10\n",
        "learning_rate = 0.0001\n",
        "model=ANN(input_size, hidden_size, output_size, learning_rate)\n",
        "model.train(X_train,one_hot_Y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOubThyQ_Dtl",
        "outputId": "65ecd17f-60bf-48ba-f67a-6ff975169c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 9.38119326197296 , Accuracy: 0.09163333333333333\n",
            "Epoch 2/100, Loss: 11.731342468736669 , Accuracy: 0.22208333333333333\n",
            "Epoch 3/100, Loss: 15.848748808551758 , Accuracy: 0.12208333333333334\n",
            "Epoch 4/100, Loss: 12.605630702518045 , Accuracy: 0.22578333333333334\n",
            "Epoch 5/100, Loss: 10.280657336251931 , Accuracy: 0.24006666666666668\n",
            "Epoch 6/100, Loss: 7.538277120460236 , Accuracy: 0.2939833333333333\n",
            "Epoch 7/100, Loss: 4.679571788606929 , Accuracy: 0.32961666666666667\n",
            "Epoch 8/100, Loss: 2.4421282065612515 , Accuracy: 0.44376666666666664\n",
            "Epoch 9/100, Loss: 1.5713168940806599 , Accuracy: 0.5191\n",
            "Epoch 10/100, Loss: 1.271567373306016 , Accuracy: 0.6044833333333334\n",
            "Epoch 11/100, Loss: 1.1446423617341086 , Accuracy: 0.63725\n",
            "Epoch 12/100, Loss: 1.0730100446537478 , Accuracy: 0.6599666666666667\n",
            "Epoch 13/100, Loss: 1.0178210669278682 , Accuracy: 0.6788833333333333\n",
            "Epoch 14/100, Loss: 0.9727335687526253 , Accuracy: 0.6942333333333334\n",
            "Epoch 15/100, Loss: 0.9343628560706294 , Accuracy: 0.7065166666666667\n",
            "Epoch 16/100, Loss: 0.901004777762014 , Accuracy: 0.71765\n",
            "Epoch 17/100, Loss: 0.8714966469902996 , Accuracy: 0.7278833333333333\n",
            "Epoch 18/100, Loss: 0.8450796865485356 , Accuracy: 0.7367166666666667\n",
            "Epoch 19/100, Loss: 0.8211876321711846 , Accuracy: 0.7453833333333333\n",
            "Epoch 20/100, Loss: 0.7994031872650263 , Accuracy: 0.7522166666666666\n",
            "Epoch 21/100, Loss: 0.7794079370613952 , Accuracy: 0.75915\n",
            "Epoch 22/100, Loss: 0.7609705343620238 , Accuracy: 0.7652833333333333\n",
            "Epoch 23/100, Loss: 0.7439178833410955 , Accuracy: 0.7708166666666667\n",
            "Epoch 24/100, Loss: 0.7281108454967541 , Accuracy: 0.7760833333333333\n",
            "Epoch 25/100, Loss: 0.7134259182980287 , Accuracy: 0.7813166666666667\n",
            "Epoch 26/100, Loss: 0.699753827207262 , Accuracy: 0.78545\n",
            "Epoch 27/100, Loss: 0.6869973624797563 , Accuracy: 0.7896333333333333\n",
            "Epoch 28/100, Loss: 0.6750695767645454 , Accuracy: 0.7935833333333333\n",
            "Epoch 29/100, Loss: 0.6638936690855454 , Accuracy: 0.79715\n",
            "Epoch 30/100, Loss: 0.6534025123664216 , Accuracy: 0.8011\n",
            "Epoch 31/100, Loss: 0.6435366813166052 , Accuracy: 0.8048\n",
            "Epoch 32/100, Loss: 0.6342419258367278 , Accuracy: 0.8075333333333333\n",
            "Epoch 33/100, Loss: 0.6254678200700393 , Accuracy: 0.81055\n",
            "Epoch 34/100, Loss: 0.6171671156629469 , Accuracy: 0.8132333333333334\n",
            "Epoch 35/100, Loss: 0.6092959353274776 , Accuracy: 0.8154333333333333\n",
            "Epoch 36/100, Loss: 0.6018141133572253 , Accuracy: 0.8172166666666667\n",
            "Epoch 37/100, Loss: 0.5946855603615144 , Accuracy: 0.8196666666666667\n",
            "Epoch 38/100, Loss: 0.5878782453352552 , Accuracy: 0.8223166666666667\n",
            "Epoch 39/100, Loss: 0.581363994003969 , Accuracy: 0.8239666666666666\n",
            "Epoch 40/100, Loss: 0.5751180837606489 , Accuracy: 0.8258166666666666\n",
            "Epoch 41/100, Loss: 0.5691188203685765 , Accuracy: 0.8276833333333333\n",
            "Epoch 42/100, Loss: 0.5633470990038091 , Accuracy: 0.8292166666666667\n",
            "Epoch 43/100, Loss: 0.5577860235484648 , Accuracy: 0.8308666666666666\n",
            "Epoch 44/100, Loss: 0.5524205695370177 , Accuracy: 0.8327333333333333\n",
            "Epoch 45/100, Loss: 0.5472373084920132 , Accuracy: 0.8346833333333333\n",
            "Epoch 46/100, Loss: 0.5422241778994412 , Accuracy: 0.83625\n",
            "Epoch 47/100, Loss: 0.5373702955838376 , Accuracy: 0.83775\n",
            "Epoch 48/100, Loss: 0.5326658072316721 , Accuracy: 0.8390833333333333\n",
            "Epoch 49/100, Loss: 0.5281017618905555 , Accuracy: 0.8402166666666666\n",
            "Epoch 50/100, Loss: 0.5236700086127156 , Accuracy: 0.8417666666666667\n",
            "Epoch 51/100, Loss: 0.5193631096778963 , Accuracy: 0.8431666666666666\n",
            "Epoch 52/100, Loss: 0.5151742664890455 , Accuracy: 0.8443166666666667\n",
            "Epoch 53/100, Loss: 0.5110972550804022 , Accuracy: 0.8454\n",
            "Epoch 54/100, Loss: 0.5071263692228802 , Accuracy: 0.8465666666666667\n",
            "Epoch 55/100, Loss: 0.5032563694879161 , Accuracy: 0.8476333333333333\n",
            "Epoch 56/100, Loss: 0.4994824374438812 , Accuracy: 0.8486\n",
            "Epoch 57/100, Loss: 0.4958001342943348 , Accuracy: 0.8498\n",
            "Epoch 58/100, Loss: 0.49220536369664836 , Accuracy: 0.85075\n",
            "Epoch 59/100, Loss: 0.48869433845326005 , Accuracy: 0.8519666666666666\n",
            "Epoch 60/100, Loss: 0.4852635508812527 , Accuracy: 0.8532666666666666\n",
            "Epoch 61/100, Loss: 0.4819097465254417 , Accuracy: 0.85475\n",
            "Epoch 62/100, Loss: 0.4786299008494856 , Accuracy: 0.8559666666666667\n",
            "Epoch 63/100, Loss: 0.4754211984352308 , Accuracy: 0.8567666666666667\n",
            "Epoch 64/100, Loss: 0.4722810142200655 , Accuracy: 0.8577333333333333\n",
            "Epoch 65/100, Loss: 0.46920689632473467 , Accuracy: 0.8585166666666667\n",
            "Epoch 66/100, Loss: 0.4661965501212175 , Accuracy: 0.8597\n",
            "Epoch 67/100, Loss: 0.4632478233093528 , Accuracy: 0.8605\n",
            "Epoch 68/100, Loss: 0.46035869189747386 , Accuracy: 0.8614\n",
            "Epoch 69/100, Loss: 0.4575272470835769 , Accuracy: 0.86235\n",
            "Epoch 70/100, Loss: 0.4547516830927944 , Accuracy: 0.86335\n",
            "Epoch 71/100, Loss: 0.4520302860461067 , Accuracy: 0.8641\n",
            "Epoch 72/100, Loss: 0.44936142392226563 , Accuracy: 0.8648833333333333\n",
            "Epoch 73/100, Loss: 0.446743537650721 , Accuracy: 0.8657\n",
            "Epoch 74/100, Loss: 0.44417513334765085 , Accuracy: 0.8664166666666666\n",
            "Epoch 75/100, Loss: 0.44165477568726375 , Accuracy: 0.8673\n",
            "Epoch 76/100, Loss: 0.43918108238114467 , Accuracy: 0.8681833333333333\n",
            "Epoch 77/100, Loss: 0.4367527197156469 , Accuracy: 0.86895\n",
            "Epoch 78/100, Loss: 0.4343683990679802 , Accuracy: 0.8696\n",
            "Epoch 79/100, Loss: 0.43202687428949177 , Accuracy: 0.8702\n",
            "Epoch 80/100, Loss: 0.4297269398160336 , Accuracy: 0.8710333333333333\n",
            "Epoch 81/100, Loss: 0.4274674293475061 , Accuracy: 0.87155\n",
            "Epoch 82/100, Loss: 0.42524721493527207 , Accuracy: 0.8723333333333333\n",
            "Epoch 83/100, Loss: 0.4230652063271535 , Accuracy: 0.87295\n",
            "Epoch 84/100, Loss: 0.4209203504414712 , Accuracy: 0.8734833333333333\n",
            "Epoch 85/100, Loss: 0.4188116308690992 , Accuracy: 0.8739666666666667\n",
            "Epoch 86/100, Loss: 0.4167380673309049 , Accuracy: 0.8745333333333334\n",
            "Epoch 87/100, Loss: 0.41469871504368794 , Accuracy: 0.8751833333333333\n",
            "Epoch 88/100, Loss: 0.41269266396882215 , Accuracy: 0.8759333333333333\n",
            "Epoch 89/100, Loss: 0.41071903793374837 , Accuracy: 0.8764666666666666\n",
            "Epoch 90/100, Loss: 0.4087769936276244 , Accuracy: 0.8769833333333333\n",
            "Epoch 91/100, Loss: 0.4068657194798485 , Accuracy: 0.8777666666666667\n",
            "Epoch 92/100, Loss: 0.4049844344349047 , Accuracy: 0.8782833333333333\n",
            "Epoch 93/100, Loss: 0.40313238664009204 , Accuracy: 0.879\n",
            "Epoch 94/100, Loss: 0.4013088520649272 , Accuracy: 0.8795833333333334\n",
            "Epoch 95/100, Loss: 0.39951313307280956 , Accuracy: 0.8801166666666667\n",
            "Epoch 96/100, Loss: 0.39774455696702005 , Accuracy: 0.8805666666666667\n",
            "Epoch 97/100, Loss: 0.39600247453416426 , Accuracy: 0.88115\n",
            "Epoch 98/100, Loss: 0.39428625860852073 , Accuracy: 0.88175\n",
            "Epoch 99/100, Loss: 0.39259530268014975 , Accuracy: 0.8822\n",
            "Epoch 100/100, Loss: 0.3909290195678463 , Accuracy: 0.8829666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/mnist_test.csv\")\n",
        "X_test=df.iloc[:,1:]\n",
        "y_test=df.iloc[:,0]\n",
        "X_test=X_test.to_numpy()/255.\n",
        "y_test=y_test.to_numpy()\n",
        "X_test.shape,y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxbHIDhXCSzM",
        "outputId": "505267a9-12f4-46f6-f343-f0fef20c7719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 784), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions = make_predictions(X_test, W1, b1, W2, b2)\n",
        "y_pred=model.predict(X_test)"
      ],
      "metadata": {
        "id": "zqKrqAb-DN8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_accuracy(y_true, y_pred_prob, k):\n",
        "    if len(y_pred_prob.shape) > 1 and y_pred_prob.shape[1] > k:\n",
        "        y_pred = np.argsort(y_pred_prob, axis=1)[:, -k:]\n",
        "    else:\n",
        "        y_pred = np.argsort(y_pred_prob, axis=1)[:, -k:]\n",
        "\n",
        "    # Check if true label is among the top-k predicted classes\n",
        "    if len(y_pred) > 0:\n",
        "        correct_predictions_top_k = np.sum(np.any(y_pred == y_true[:, np.newaxis], axis=1))\n",
        "    else:\n",
        "        correct_predictions_top_k = 0\n",
        "\n",
        "    # Calculate accuracy\n",
        "    total_samples = len(y_true)\n",
        "    accuracy_top_k = correct_predictions_top_k / total_samples\n",
        "    return accuracy_top_k\n",
        "\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred, class_label=1):\n",
        "    true_positives = np.sum((y_true == class_label) & (y_pred == class_label))\n",
        "    predicted_positives = np.sum(y_pred == class_label)\n",
        "    return true_positives / predicted_positives\n",
        "\n",
        "def recall(y_true, y_pred, class_label=1):\n",
        "    true_positives = np.sum((y_true == class_label) & (y_pred == class_label))\n",
        "    actual_positives = np.sum(y_true == class_label)\n",
        "    return true_positives / actual_positives\n",
        "\n",
        "def f1_score(y_true, y_pred, class_label=1):\n",
        "    prec = precision(y_true, y_pred, class_label)\n",
        "    rec = recall(y_true, y_pred, class_label)\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def confusion_matrix_custom(y_true, y_pred):\n",
        "    unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
        "    matrix = np.zeros((len(unique_labels), len(unique_labels)), dtype=int)\n",
        "    for i, true_label in enumerate(unique_labels):\n",
        "        for j, pred_label in enumerate(unique_labels):\n",
        "            matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
        "    return matrix\n",
        "\n",
        "def classification_metric(y_test, y_pred):\n",
        "    print(f'''\n",
        "Top 5 Accuracy :{top_k_accuracy(y_test, y_pred, 5):.3f}\n",
        "Top 1 Accuracy :{top_k_accuracy(y_test, y_pred, 1):.3f}\n",
        "# Classication Report:\n",
        "# Precision(class 0): {precision(y_test, np.argmax(y_pred, axis=1) , class_label=0):.2f}\n",
        "# Recall(class 0)   : {recall(y_test, np.argmax(y_pred, axis=1) , class_label=0):.2f}\n",
        "# F1-score(class 0) : {f1_score(y_test, np.argmax(y_pred, axis=1) , class_label=0):.2f}\n",
        "\n",
        "# Precision(class 1): {precision(y_test,np.argmax(y_pred, axis=1) , class_label=1):.2f}\n",
        "# Recall(class 1)   : {recall(y_test, np.argmax(y_pred, axis=1) , class_label=1):.2f}\n",
        "# F1-score(class 1) : {f1_score(y_test, np.argmax(y_pred, axis=1) , class_label=1):.2f}\n",
        "\n",
        "Confusion Matrix:\n",
        "{confusion_matrix_custom(y_test,np.argmax(y_pred, axis=1) )}\n",
        "''')\n",
        "classification_metric( y_test,y_pred)"
      ],
      "metadata": {
        "id": "nFi1FWgZd68R",
        "outputId": "b1a66d16-15c0-43fd-aa01-7a8f8c9960f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Accuracy :0.991\n",
            "Top 1 Accuracy :0.881\n",
            "# Classication Report:\n",
            "# Precision(class 0): 0.92\n",
            "# Recall(class 0)   : 0.94\n",
            "# F1-score(class 0) : 0.93\n",
            "\n",
            "# Precision(class 1): 0.97\n",
            "# Recall(class 1)   : 0.97\n",
            "# F1-score(class 1) : 0.97\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 924    0    4    3    3   13   17    6    9    1]\n",
            " [   0 1098    8    5    0    3    4    0   16    1]\n",
            " [  13    3  880   23   18   10   20   17   43    5]\n",
            " [   9    2   27  885    2   37    5    9   23   11]\n",
            " [   4    2    8    3  858    5   15    7   12   68]\n",
            " [  13    3    5   53    9  742   20    7   31    9]\n",
            " [  22    5   11    1   18   22  869    0   10    0]\n",
            " [   0   14   28    6    7    5    0  900    5   63]\n",
            " [  13    5   23   43   17   25   22   12  800   14]\n",
            " [  10    3    3   11   59   13    1   38   16  855]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibBOaOCXEUds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}